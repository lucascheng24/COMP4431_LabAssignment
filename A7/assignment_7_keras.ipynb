{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucascheng24/COMP4431_LabAssignment/blob/main/A7/assignment_7_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PiwCFfg9imD"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from typing import Any, Tuple, Optional, Callable\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "def read_csv(path: str) -> pd.DataFrame:\n",
        "    '''\n",
        "    Read a csv file.\n",
        "\n",
        "    Args:\n",
        "        path (str): Path to the csv file.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Dataframe with the csv file data.\n",
        "    '''\n",
        "\n",
        "    assert os.path.exists(path), f'CSV file not found: {path}!'\n",
        "    assert os.path.splitext(path)[\n",
        "    -1] == '.csv', f'Unsupported file type {os.path.splitext(path)[-1]}!'\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, dataframe: pd.DataFrame, images_folder: str = './images', transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None:\n",
        "        '''\n",
        "        Image dataset.\n",
        "\n",
        "        Args:\n",
        "            dataframe (pd.DataFrame): Dataframe with the image filenames and labels.\n",
        "            images_folder (str): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "            target_transform (callable, optional): Optional transform to be applied on a target.\n",
        "        '''\n",
        "        assert 'Filename' in dataframe.columns, f'Filename column not found!'\n",
        "        assert os.path.exists(images_folder), f'Image folder not found: {images_folder}!'\n",
        "\n",
        "        self.dataframe = dataframe\n",
        "        self.images_folder = images_folder\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        data = []\n",
        "        targets = []\n",
        "\n",
        "        for i, sample in dataframe.iterrows():\n",
        "            image = cv2.imread(os.path.join(images_folder, sample['Filename']))\n",
        "            data.append(image)\n",
        "\n",
        "            targets.append(int(sample['Label']) if 'Label' in sample else -1)\n",
        "\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        '''\n",
        "        Returns:\n",
        "            int: Length of the dataset.\n",
        "        '''\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
        "        '''\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is class_index of the target class. For the public test set, target is a class from [0, 1, 2, 3, 4, 5, 6, 7, 8]. For the private test set (before releasing the test set labels), target is -1.\n",
        "        '''\n",
        "        img = self.data[index]\n",
        "        target = self.targets[index]\n",
        "\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "ydqBmAmZ9imF",
        "outputId": "959cb99b-9e84-49c0-9d03-b99171300df8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image <class 'PIL.Image.Image'> (28, 28)\n",
            "Target <class 'int'>\n",
            "Length 85744\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\nCODE HERE!\\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "public_dataframe = read_csv('assignment_7_public.csv')\n",
        "public_dataset = ImageDataset(public_dataframe)\n",
        "\n",
        "print('Image', type(public_dataset[0][0]), public_dataset[0][0].size) # Image <class 'PIL.Image.Image'> (28, 28)\n",
        "print('Target', type(public_dataset[0][1])) # Target <class 'int'>\n",
        "print('Length', len(public_dataset)) # Length 85744\n",
        "\n",
        "'''\n",
        "CODE HERE!\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wIhxK9VYDUK",
        "outputId": "2ef00a79-5a19-4974-dee0-c696aed72a86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "536/536 [==============================] - 3s 6ms/step - loss: 1.5319 - accuracy: 0.4261 - val_loss: 1.1533 - val_accuracy: 0.5709\n",
            "Epoch 2/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 1.1437 - accuracy: 0.5798 - val_loss: 0.9393 - val_accuracy: 0.6635\n",
            "Epoch 3/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 1.0113 - accuracy: 0.6316 - val_loss: 0.8467 - val_accuracy: 0.6997\n",
            "Epoch 4/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.9303 - accuracy: 0.6610 - val_loss: 0.8401 - val_accuracy: 0.6971\n",
            "Epoch 5/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.8851 - accuracy: 0.6786 - val_loss: 0.7510 - val_accuracy: 0.7283\n",
            "Epoch 6/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.8586 - accuracy: 0.6873 - val_loss: 0.7660 - val_accuracy: 0.7186\n",
            "Epoch 7/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.8182 - accuracy: 0.7037 - val_loss: 0.7262 - val_accuracy: 0.7379\n",
            "Epoch 8/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.7865 - accuracy: 0.7141 - val_loss: 0.7680 - val_accuracy: 0.7062\n",
            "Epoch 9/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.7755 - accuracy: 0.7167 - val_loss: 0.7589 - val_accuracy: 0.7167\n",
            "Epoch 10/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.7650 - accuracy: 0.7232 - val_loss: 0.6637 - val_accuracy: 0.7575\n",
            "Epoch 11/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.7444 - accuracy: 0.7291 - val_loss: 0.6343 - val_accuracy: 0.7670\n",
            "Epoch 12/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.7234 - accuracy: 0.7360 - val_loss: 0.6122 - val_accuracy: 0.7767\n",
            "Epoch 13/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.7198 - accuracy: 0.7369 - val_loss: 0.6771 - val_accuracy: 0.7530\n",
            "Epoch 14/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.7006 - accuracy: 0.7440 - val_loss: 0.6274 - val_accuracy: 0.7700\n",
            "Epoch 15/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.6823 - accuracy: 0.7522 - val_loss: 0.5849 - val_accuracy: 0.7840\n",
            "Epoch 16/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.6821 - accuracy: 0.7510 - val_loss: 0.6042 - val_accuracy: 0.7779\n",
            "Epoch 17/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.6622 - accuracy: 0.7586 - val_loss: 0.5717 - val_accuracy: 0.7869\n",
            "Epoch 18/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.6555 - accuracy: 0.7614 - val_loss: 0.5756 - val_accuracy: 0.7861\n",
            "Epoch 19/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.6424 - accuracy: 0.7664 - val_loss: 0.5707 - val_accuracy: 0.7932\n",
            "Epoch 20/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.6441 - accuracy: 0.7642 - val_loss: 0.6155 - val_accuracy: 0.7786\n",
            "Epoch 21/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.6169 - accuracy: 0.7749 - val_loss: 0.5605 - val_accuracy: 0.7951\n",
            "Epoch 22/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.6160 - accuracy: 0.7774 - val_loss: 0.5665 - val_accuracy: 0.7856\n",
            "Epoch 23/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.6081 - accuracy: 0.7781 - val_loss: 0.5913 - val_accuracy: 0.7818\n",
            "Epoch 24/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.5984 - accuracy: 0.7821 - val_loss: 0.5460 - val_accuracy: 0.8036\n",
            "Epoch 25/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.5908 - accuracy: 0.7869 - val_loss: 0.5139 - val_accuracy: 0.8109\n",
            "Epoch 26/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.5825 - accuracy: 0.7876 - val_loss: 0.5850 - val_accuracy: 0.7858\n",
            "Epoch 27/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.5819 - accuracy: 0.7890 - val_loss: 0.5155 - val_accuracy: 0.8135\n",
            "Epoch 28/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.5754 - accuracy: 0.7922 - val_loss: 0.5413 - val_accuracy: 0.8033\n",
            "Epoch 29/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.5576 - accuracy: 0.7989 - val_loss: 0.5130 - val_accuracy: 0.8094\n",
            "Epoch 30/30\n",
            "536/536 [==============================] - 3s 5ms/step - loss: 0.5506 - accuracy: 0.8002 - val_loss: 0.5380 - val_accuracy: 0.8050\n",
            "536/536 [==============================] - 1s 1ms/step - loss: 0.5380 - accuracy: 0.8050\n",
            "Accuracy: 0.8050032258033752\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Assuming the label column is named 'label' in public_dataframe\n",
        "labels = np.array(public_dataframe['Label'])\n",
        "\n",
        "# Extract the image data from the ImageDataset\n",
        "images = np.asarray([np.array(sample[0]) for sample in public_dataset])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the input data (assuming it is in the range of 0-255)\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "input_shape = (28, 28, 3)\n",
        "\n",
        "# Define the model architecture\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        keras.layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dropout(0.5),\n",
        "        keras.layers.Dense(9, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.compile(optimizer='Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 30\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "_, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz63m9MD9imF",
        "outputId": "895e9450-6a41-4091-f735-59b7e04affe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image <class 'PIL.Image.Image'>\n",
            "Length 21436\n",
            "670/670 [==============================] - 1s 1ms/step\n"
          ]
        }
      ],
      "source": [
        "private_dataframe = read_csv('assignment_7_private.csv')\n",
        "private_dataset = ImageDataset(private_dataframe)\n",
        "\n",
        "print('Image', type(private_dataset[0][0])) # Image <class 'PIL.Image.Image'> (28, 28)\n",
        "print('Length', len(private_dataset)) # Length 21436\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# remove and make your own predictions.\n",
        "preds = np.full(len(private_dataset), -1,\n",
        "                dtype=int)\n",
        "'''\n",
        "CODE HERE!\n",
        "e.g.,\n",
        "preds = np.full(len(X_private), -1, dtype=int)\n",
        "'''\n",
        "\n",
        "# Convert images to numpy arrays and normalize\n",
        "predict_imgages = np.asarray([np.array(sample[0]) for sample in private_dataset])\n",
        "predict_imgages = predict_imgages.astype('float32') / 255.0\n",
        "\n",
        "# Make predictions using the trained model\n",
        "predicted_labels = model.predict(predict_imgages)\n",
        "\n",
        "# Get the predicted labels\n",
        "preds = np.argmax(predicted_labels, axis=1)\n",
        "\n",
        "\n",
        "submission = pd.DataFrame({'Label': preds})\n",
        "submission.to_csv('assignment_7.csv', index=True, index_label='Id')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}