{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from typing import Any, Tuple, Optional, Callable\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(path: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Read a csv file.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the csv file.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with the csv file data.\n",
    "    '''\n",
    "    \n",
    "    assert os.path.exists(path), f'CSV file not found: {path}!'\n",
    "    assert os.path.splitext(path)[\n",
    "    -1] == '.csv', f'Unsupported file type {os.path.splitext(path)[-1]}!'\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, images_folder: str = './images', transform: Optional[Callable] = None, target_transform: Optional[Callable] = None) -> None:\n",
    "        '''\n",
    "        Image dataset.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): Dataframe with the image filenames and labels.\n",
    "            images_folder (str): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            target_transform (callable, optional): Optional transform to be applied on a target.\n",
    "        '''\n",
    "        assert 'Filename' in dataframe.columns, f'Filename column not found!'\n",
    "        assert os.path.exists(images_folder), f'Image folder not found: {images_folder}!'\n",
    "\n",
    "        self.dataframe = dataframe\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        data = []\n",
    "        targets = []\n",
    "\n",
    "        for i, sample in dataframe.iterrows():\n",
    "            image = cv2.imread(os.path.join(images_folder, sample['Filename']))\n",
    "            data.append(image)\n",
    "\n",
    "            targets.append(int(sample['Label']) if 'Label' in sample else -1)\n",
    "\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        '''\n",
    "        Returns:\n",
    "            int: Length of the dataset.\n",
    "        '''\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        '''\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (image, target) where target is class_index of the target class. For the public test set, target is a class from [0, 1, 2, 3, 4, 5, 6, 7, 8]. For the private test set (before releasing the test set labels), target is -1.\n",
    "        '''\n",
    "        img = self.data[index]\n",
    "        target = self.targets[index]\n",
    "\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU/MPS if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "#     elif torch.backends.mps.is_available():\n",
    "#         return torch.device('mps')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "\n",
    "def show_batch(dl):\n",
    "    for images, labels in dl:\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_dataframe = read_csv('assignment_7_public.csv')\n",
    "train_split, valid_split = train_test_split(public_dataframe, test_size=0.1, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image <class 'PIL.Image.Image'> (28, 28)\n",
      "Target <class 'int'>\n",
      "Length 85744\n",
      "\n",
      "mean RGB: 0.73970443 0.5331324 0.70590323\n",
      "sd RGB: 0.12986475 0.175617 0.13220353\n"
     ]
    }
   ],
   "source": [
    "public_dataset = ImageDataset(public_dataframe)\n",
    "\n",
    "print('Image', type(public_dataset[0][0]), public_dataset[0][0].size) # Image <class 'PIL.Image.Image'> (28, 28)\n",
    "print('Target', type(public_dataset[0][1])) # Target <class 'int'>\n",
    "print('Length', len(public_dataset)) # Length 85744\n",
    "\n",
    "imgs = [to_tensor(item[0]) for item in public_dataset] # item[0] and item[1] are image and its label\n",
    "imgs = torch.stack(imgs, dim=0).numpy()\n",
    "\n",
    "# calculate mean over each channel (r,g,b)\n",
    "mean_r = imgs[:,0,:,:].mean()\n",
    "mean_g = imgs[:,1,:,:].mean()\n",
    "mean_b = imgs[:,2,:,:].mean()\n",
    "print('\\nmean RGB:', mean_r,mean_g,mean_b)\n",
    "\n",
    "# calculate std over each channel (r,g,b)\n",
    "std_r = imgs[:,0,:,:].std()\n",
    "std_g = imgs[:,1,:,:].std()\n",
    "std_b = imgs[:,2,:,:].std()\n",
    "print('sd RGB:', std_r,std_g,std_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = ((mean_r, mean_g, mean_b), (std_r, std_g, std_b))\n",
    "train_transforms = transforms.Compose([transforms.RandomCrop(32, padding=4, padding_mode='reflect'), \n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(), \n",
    "                                       transforms.Normalize(*stats,inplace=True)])\n",
    "test_transforms = transforms.Compose([transforms.ToTensor(), \n",
    "                                      transforms.Normalize(*stats)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 400\n",
    "num_workers = 3\n",
    "classes = (0, 1, 2, 3, 4, 5, 6, 7, 8)\n",
    "\n",
    "\n",
    "train_dataset = ImageDataset(dataframe=train_split, transform=train_transforms)\n",
    "train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "\n",
    "valid_dataset = ImageDataset(dataframe=valid_split, transform=test_transforms)\n",
    "valid_dl = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "device_count: 1\n",
      "GPU Device: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# define to use cpu or gpu\n",
    "device = get_default_device()\n",
    "print('device:', device)\n",
    "print('device_count:', torch.cuda.device_count())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(device)\n",
    "    print('GPU Device:', device_name)\n",
    "else:\n",
    "    print('CPU Device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "valid_dl = DeviceDataLoader(valid_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dl.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(valid_dl.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels, pool=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
    "              nn.BatchNorm2d(out_channels), \n",
    "              nn.ReLU(inplace=True)]\n",
    "    if pool: layers.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ResNet9(ImageClassificationBase):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = conv_block(in_channels, 16) \n",
    "        self.conv2 = conv_block(16, 32, pool=True) \n",
    "        self.res1 = nn.Sequential(conv_block(32, 32), conv_block(32, 32)) \n",
    "        \n",
    "        self.conv3 = conv_block(32, 64, pool=True) \n",
    "        self.conv4 = conv_block(64, 128, pool=True) \n",
    "        self.res2 = nn.Sequential(conv_block(128, 128), conv_block(128, 128)) \n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(4), \n",
    "                                        nn.Flatten(), \n",
    "                                        nn.Linear(128, num_classes)) \n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet9(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (res1): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (res2): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = to_device(ResNet9(3, 10), device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413514\n"
     ]
    }
   ],
   "source": [
    "trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(trainable_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    total_batches = len(val_loader)\n",
    "    progress_bar = tqdm(total=total_batches, desc='Evaluation Progress')\n",
    "\n",
    "    for batch in val_loader:\n",
    "        output = model.validation_step(batch)\n",
    "        outputs.append(output)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    progress_bar.close()\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    # Set up cutom optimizer with weight decay\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Record & update learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cannot eval the dataset, run 40 mins without any progress update (suppose to have 22 stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Progress:   0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "history = [evaluate(model, valid_dl)]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "epochs = 5\n",
    "max_lr = 0.01\n",
    "grad_clip = 0.1\n",
    "weight_decay = 1e-4\n",
    "opt_func = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y\n"
     ]
    }
   ],
   "source": [
    "history = None  # Define and initialize 'history'\n",
    "if history is None: print('Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, \n",
    "                             grad_clip=grad_clip, \n",
    "                             weight_decay=weight_decay, \n",
    "                             opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './clf01/CNN_net.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(history):\n",
    "    accuracies = [x['val_acc'] for x in history]\n",
    "    plt.plot(accuracies, '-x')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Accuracy vs. No. of epochs')\n",
    "\n",
    "def plot_losses(history):\n",
    "    train_losses = [x.get('train_loss') for x in history]\n",
    "    val_losses = [x['val_loss'] for x in history]\n",
    "    plt.plot(train_losses, '-bx')\n",
    "    plt.plot(val_losses, '-rx')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.title('Loss vs. No. of epochs')\n",
    "\n",
    "def plot_lrs(history):\n",
    "    lrs = np.concatenate([x.get('lrs', []) for x in history])\n",
    "    plt.plot(lrs)\n",
    "    plt.xlabel('Batch no.')\n",
    "    plt.ylabel('Learning rate')\n",
    "    plt.title('Learning Rate vs. Batch no.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracies(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lrs(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
